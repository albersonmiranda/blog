[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Pacote R projetado para análise de insumo-produto, enfatizando a usabilidade para usuários de Excel e desempenho. Inclui um Addin para RStudio e um conjunto de funções para importação simplificada de tabelas de insumo-produto do Excel, seja programaticamente ou diretamente da área de transferência.\nO pacote é otimizado para velocidade e eficiência. Ele utiliza a classe R6 para programação orientada a objetos limpa e eficiente em termos de memória. Além disso, todos os cálculos de álgebra linear são implementados em Rust para alcançar um desempenho altamente otimizado.\n Código  Site"
  },
  {
    "objectID": "software.html#fio",
    "href": "software.html#fio",
    "title": "Software",
    "section": "",
    "text": "Pacote R projetado para análise de insumo-produto, enfatizando a usabilidade para usuários de Excel e desempenho. Inclui um Addin para RStudio e um conjunto de funções para importação simplificada de tabelas de insumo-produto do Excel, seja programaticamente ou diretamente da área de transferência.\nO pacote é otimizado para velocidade e eficiência. Ele utiliza a classe R6 para programação orientada a objetos limpa e eficiente em termos de memória. Além disso, todos os cálculos de álgebra linear são implementados em Rust para alcançar um desempenho altamente otimizado.\n Código  Site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Todos os modelos estão errados, mas alguns são inúteis\n\n\nParte I: Correlação de Pearson e significância\n\n\n\ntime-series\n\n\n\n\n\n\n\n\n\n19 de abr. de 2025\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nSéries Temporais Hierárquicas: teoria\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n25 de out. de 2022\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nO Papel Da Educação Matemática Sob A Hegemonia Do Capital\n\n\n\n\n\n\neducation\n\n\n\n\n\n\n\n\n\n14 de ago. de 2022\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate para dissertações em Quarto\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n3 de ago. de 2022\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nPython a partir do R I: importando pacotes (e por que aprender novas linguagens é difícil)\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n12 de jun. de 2021\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nUm pouco de conceitos: overfitting & resampling\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n17 de abr. de 2021\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nR em 2021 com o VSCode\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n6 de abr. de 2021\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nIntrodução ao framework {mlr3}\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n27 de dez. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nEffect Size e a desigualdade de renda entre gêneros em Vitória\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n24 de nov. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nComparando variâncias: o teste F\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n20 de nov. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nChutou ou não chutou? O test t para uma amostra\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n6 de nov. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nAtestando diferenças em médias: o teste t para amostras independentes\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n2 de nov. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nQualidade gráfica no painel do RStudio\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n19 de out. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nConfigurando git em um servidor proxy\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n25 de set. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\n\n\n\n\n\n\nHello World\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n11 de set. de 2020\n\n\nAlberson Miranda\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "posts/202011_introducao-ao-mlr3-framework/index.html",
    "href": "posts/202011_introducao-ao-mlr3-framework/index.html",
    "title": "Introdução ao framework {mlr3}",
    "section": "",
    "text": "Esse é o primeiro post de uma série que irá tratar sobre o ecossistema {mlr3} (Lang et al. 2019). Ele é mais completo e também muito mais complexo do que seu predecessor, o {mlr}, que teve sua versão inicial publicada no CRAN em 2013. O ecossistema permite um framework agnóstico (i.e. não depende dos algoritmos escolhidos), extensível e orientado a objeto, e, atualmente, permite vários tipos de tarefas, como classificação, regressão, análise de sobrevivência, forecasting, clustering, dentre outros. O {mlr3} tem diversas vantagens que o faz, IMHO, o framework mais completo para se trabalhar machine learning em R (R Core Team 2020) e elas ficarão claras ao longo dos próximos posts."
  },
  {
    "objectID": "posts/202011_introducao-ao-mlr3-framework/index.html#características-de-design",
    "href": "posts/202011_introducao-ao-mlr3-framework/index.html#características-de-design",
    "title": "Introdução ao framework {mlr3}",
    "section": "1.1 CARACTERÍSTICAS DE DESIGN",
    "text": "1.1 CARACTERÍSTICAS DE DESIGN\nAlguns princípios gerais que norteiam o desenvolvimento do pacote e afetam muito seu uso são:\n\nFoco no backend. A maioria dos pacotes do ecossistema tem objetivo de processar e transformar dados, aplicar algoritmos e computar resultados. Visualizações são providenciadas em pacotes externos;\nAdoção da classe R6 (Chang 2020) para design orientado a objeto, modify-in-place e semântica de referência (falaremos um pouco sobre esses conceitos adiante);\nAdoção do {data.table} (Dowle e Srinivasan 2020) para manipulações de data frames. A combinação {R6} + {data.table} torna a performance um dos pontos fortes do ecossistema.\nBaixa dependência. Entretanto, os algortimos não são implementados no ecossistema, como no scikit-learn em Python. Para executar o XGBoost (Chen et al. 2020), por exemplo, deve-se ter instalado o pacote que o implementa."
  },
  {
    "objectID": "posts/202011_introducao-ao-mlr3-framework/index.html#fora-do-escopo",
    "href": "posts/202011_introducao-ao-mlr3-framework/index.html#fora-do-escopo",
    "title": "Introdução ao framework {mlr3}",
    "section": "1.2 FORA DO ESCOPO",
    "text": "1.2 FORA DO ESCOPO\nComo se trata de uma introdução, as etapas de tunning e resampling, assim como funcionalidades como os pipelines, serão abordadas em postagens futuras. Neste post trataremos apenas os conceitos básicos do workflow."
  },
  {
    "objectID": "posts/202011_introducao-ao-mlr3-framework/index.html#footnotes",
    "href": "posts/202011_introducao-ao-mlr3-framework/index.html#footnotes",
    "title": "Introdução ao framework {mlr3}",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nNo {mlr3} é chamado de learner.↩︎\nTambém chamada de output feature ou label.↩︎\nAqui trabalharemos apenas com um, mas em posts futuros utilizaremos de diversas formas — pipelines com diferentes features, stacking etc.↩︎\nRaiz do Erro Médio Quadrático↩︎"
  },
  {
    "objectID": "posts/202101_overfitting-resampling/index.html",
    "href": "posts/202101_overfitting-resampling/index.html",
    "title": "Um pouco de conceitos: overfitting & resampling",
    "section": "",
    "text": "Apesar de serem termos recorrentes em machine learning, resampling e overfitting são frequentemente discutidos apenas na prática, muitas vezes sem a sua compreensão. Neste post1, tentarei introduzir os conceitos de forma genérica."
  },
  {
    "objectID": "posts/202101_overfitting-resampling/index.html#footnotes",
    "href": "posts/202101_overfitting-resampling/index.html#footnotes",
    "title": "Um pouco de conceitos: overfitting & resampling",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nTotalmente apoiado em (Bischl B. 2012).↩︎"
  },
  {
    "objectID": "posts/202010_git-proxy/index.html",
    "href": "posts/202010_git-proxy/index.html",
    "title": "Configurando git em um servidor proxy",
    "section": "",
    "text": "Se você estiver trabalhando em uma organização que leva a sério a segurança da informação, então, provavelmente, você está atrás de um servidor proxy e com dificuldades para usar o Git. Para resolvermos isso, precisaremos de passar por 3 etapas:"
  },
  {
    "objectID": "posts/202010_git-proxy/index.html#gitconfig-sem-salvar-senha",
    "href": "posts/202010_git-proxy/index.html#gitconfig-sem-salvar-senha",
    "title": "Configurando git em um servidor proxy",
    "section": "2.1 .gitconfig sem salvar senha",
    "text": "2.1 .gitconfig sem salvar senha\nRecomendo usar o pacote {usethis} para alterar qualquer arquivo de configuração no R. Para a primeira opção, faríamos da seguinte maneira:\n\n# abrir o arquivo de configuração\nusethis::edit_git_config()\n\nNa janela do .gitconfig que será aberta, adicione as seguintes linhas:\n[http]\n    proxy = http[s]://dominio.com:porta\n\n[credential]\n    helper = wincred\n\n[credential \"helperselector\"]\n    selected = manager\n\nEm que “domínio.com” é o endereço proxy que você achou no arquivo .dat e as demais configurações definem a forma como você será solicitado a incluir usuário e senha, nesse caso através de uma janela pop up a cada push/pull."
  },
  {
    "objectID": "posts/202010_git-proxy/index.html#gitconfig-com-senha-gravada",
    "href": "posts/202010_git-proxy/index.html#gitconfig-com-senha-gravada",
    "title": "Configurando git em um servidor proxy",
    "section": "2.2 .gitconfig com senha gravada",
    "text": "2.2 .gitconfig com senha gravada\nA outra alternativa é gravar o usuário e senha no próprio .gitconfig. Novamente, se o arquivo ficar em rede ou se outras pessoas tiverem acesso à máquina, evite essa opção. Para deixar seu usuário e senha salvas no .gitconfig, basta adicioná-los antes do domínio. A vantagem desse método a não ter de inserir as informações a cada push/pull.\n[http]\n    proxy = http[s]://usuario:senha@dominio.com:porta\n\nLembre-se de atualizar a senha no .gitconfig sempre que ela for alterada!"
  },
  {
    "objectID": "posts/202010_git-proxy/index.html#footnotes",
    "href": "posts/202010_git-proxy/index.html#footnotes",
    "title": "Configurando git em um servidor proxy",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nEvite o bloco de notas para exibir corretamente as quebras de linha. Sugiro o Wordpad.↩︎"
  },
  {
    "objectID": "posts/202010_graphics-rstudio/index.html",
    "href": "posts/202010_graphics-rstudio/index.html",
    "title": "Qualidade gráfica no painel do RStudio",
    "section": "",
    "text": "Se você já plotou um gráfico de linhas no R e, quando viu o plot no painel do RStudio pensou “uau, que qualidade terrível!”, você não está só. Mas não se preocupe, a solução é bem simples!\nPrimeiramente, vamos plotar um gráfico na configuração padrão do RStudio:\n\nknitr::include_graphics(\"img/Rplot.png\")\n\n\n\n\n\n\n\nFigura 1: Gráfico sem anti-aliasing.\n\n\n\n\n\nWow, imagine usando algo assim em um banner! Vamos tentar de novo, agora com o Cairo como dispositivo gráfico e usando anti-aliasing:\n\n# Adicionando anti-aliasing\ntrace(grDevices::png, quote({\n  if (missing(type) && missing(antialias)) {\n    type = \"cairo-png\"\n    antialias = \"subpixel\"\n  }\n}), print = FALSE)\n\nTracing function \"png\" in package \"grDevices\"\n\n\n[1] \"png\"\n\n# Plotando novamente\nplot(mtcars$mpg, type = \"l\")\n\n\n\n\n\n\n\nFigura 2: Gráfico com anti-aliasing.\n\n\n\n\n\nSanto anti-aliasing, não é? (Dê zoom nos dois para ver bem a diferença)\nBom, preciso fazer isso toda vez que for plotar um gráfico ou iniciar uma sessão no R? De forma alguma, basta adicionar essas linhas ao seu .Rprofile. Eu recomendo sempre usar o {usethis} para alterar os arquivos de configuração, pois você pode acabar se perdendo dentre os caminhos possíveis onde o R irá consultar. Usando o {usethis} você garante que estará criando ou editando o arquivo correto.\nusethis::edit_r_profile() irá abrir uma janela com o arquivo para edição. Então, cole a chamada acima (retirando a linha do plot, claro), certifique-se que o .Rprofile acabe com uma linha vazia (porque o R ignora a última linha), salve e reinicie a sessão. Pronto! Agora o anti-aliasing sempre será aplicado aos seus plots no R, independente do pacote utilizado, seja o base ou o {ggplot}, por exemplo."
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html",
    "href": "posts/202011_chutou-ou-nao/index.html",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "",
    "text": "No post anterior, falei sobre o teste t para duas amostras independentes. Coincidentemente, no dia seguinte apareceu essa dúvida no fórum de matemática do qual sou contribuidor. Não podemos perder essa oportunidade, não é mesmo?"
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#declarar-as-hipóteses-nula-e-alternativa",
    "href": "posts/202011_chutou-ou-nao/index.html#declarar-as-hipóteses-nula-e-alternativa",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "3.1 Declarar as hipóteses nula e alternativa",
    "text": "3.1 Declarar as hipóteses nula e alternativa\n\\[\\begin{cases}\n      H_0: \\mu = 0.25 \\\\\n      H_1: \\mu \\neq 0.25 \\\\\n    \\end{cases}\\]\nA hipótese nula é que não se pode afirmar que a média de acerto observada é significativamente diferente da média esperada. Já a hipótese alternativa é que elas são significativamente distintas."
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#declarar-o-nível-de-significância",
    "href": "posts/202011_chutou-ou-nao/index.html#declarar-o-nível-de-significância",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "3.2 Declarar o nível de significância",
    "text": "3.2 Declarar o nível de significância\n\\[\\alpha = 0.02\\] O \\(\\alpha = 0.02\\) é o que dá sentido ao termo significantemente diferente. É a probabilidade de se cometer o erro do tipo II, ou seja, rejeitar a hipótese nula quando não deveria ser rejeitada. Quanto menor o \\(\\alpha\\), maior deverá ser a diferença entre as médias para que ela seja considerada significante."
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#calcular-a-estatística-do-teste",
    "href": "posts/202011_chutou-ou-nao/index.html#calcular-a-estatística-do-teste",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "3.3 Calcular a estatística do teste",
    "text": "3.3 Calcular a estatística do teste\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] Note que: \\[\\lim_{n \\to \\infty}z(n) = \\infty\\] Podemos rejeitar a hipótese nula se \\(z\\) crítico for maior que o \\(z\\) tabelado. Conforme \\(n\\) aumenta, eventualmente a diferença será significativa, demonstrando matematicamente o que verificamos intuitivamente."
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#calcular-o-valor-crítico",
    "href": "posts/202011_chutou-ou-nao/index.html#calcular-o-valor-crítico",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "3.4 Calcular o valor crítico",
    "text": "3.4 Calcular o valor crítico\n\\[z = \\frac{0.3125 - 0.25}{\\frac{0.464}{\\sqrt{400}}} = 2.693\\]"
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#decidir-se-rejeitamos-a-hipótese-nula",
    "href": "posts/202011_chutou-ou-nao/index.html#decidir-se-rejeitamos-a-hipótese-nula",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "3.5 Decidir se rejeitamos a hipótese nula",
    "text": "3.5 Decidir se rejeitamos a hipótese nula\nComo o valor de 2.693 excede o valor de \\(z\\) a 98% de significância (2.33), pode-se rejeitar a hipótese nula. A diferença é significativa e não pode ser atribuída ao acaso de amostragem."
  },
  {
    "objectID": "posts/202011_chutou-ou-nao/index.html#footnotes",
    "href": "posts/202011_chutou-ou-nao/index.html#footnotes",
    "title": "Chutou ou não chutou? O test t para uma amostra",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nOu mil, 10 mil, 100 mil… Quanto maior for \\(n\\), mais difícil será causar mudanças no desvio padrão, de forma que a incerteza é cada vez menor.↩︎"
  },
  {
    "objectID": "posts/202208_educacao-pre-capitalista/index.html",
    "href": "posts/202208_educacao-pre-capitalista/index.html",
    "title": "O Papel Da Educação Matemática Sob A Hegemonia Do Capital",
    "section": "",
    "text": "Nota\n\n\n\nEsse post foi tirado de um capítulo de minha monografia de graduação em Matemática, defendida em 2024.\nEm uma sociedade capitalista, não se pode compreender a educação desassociada das relações de trabalho. Pois além das técnicas aplicadas à educação – aí compreendidas a pedagogia em suas dimensões filosófica e sociológica –, as normas vigentes e valores compartilhados de uma determinada sociedade também são refletidas e moldam as ações educacionais.\nNeste capítulo, procuro sistematizar os mecanismos de reprodução do capitalismo na educação, mostrando evidenciar como a educação matemática, enquanto não rompe com seu papel na reprodução das relações sociais de poder, é ineficaz na promoção de mudanças estruturais na sociedade.\nConsidere o seguinte exercício mental: Suponha uma sociedade em um futuro muito distante onde não exista trabalho como relação social. Robôs realizam toda a atividade laboral. Como seria a escola nessa sociedade? O que sobra de puro na educação? Qual seria seu propósito? Antes de tentar inferir qualquer coisa acerca de tais perguntas, podemos nos munir de uma breve visão geral do propósito da educação na história da educação.\nDe acordo com Cubberley (1920), na Atenas do século V a.C., antes do tempo dos sofistas, o currículo básico consistia de leitura, escrita, música e ginástica, e era requerido para a obtenção do status de cidadão. Somente detentores desse grau eram autorizados a participar das ekklesia, a principal assembléia da democracia ateniense. A educação, a qual era exclusiva para homens, era privada e as taxas dependiam da capacidade de pagamento dos pais. Apenas professores de grandes escolas tinham algum prestígio, com os demais ocupando posições baixas na hierarquia social ateniense. Gramática, aritimética, ciências ou línguas estrangeiras não faziam parte do currículo – mas apenas aquilo que era necessário para a normalização moral do indivíduo ateniense: Música, literatura, sua própria religião, treinamento físico e instruções acerca das tarefas e obrigações de um cidadão.\nSegundo o autor, as fábulas de Homero recheadas de heroismo, Ilíada e Odisséia, eram as primeiras e maiores leituras dos gregos, de forma que “To appeal to the emotions and to stir the will along moral and civic lines was a fundamental purpose of the instruction”. Nessas obras, estavam incluídas lições de ética, política, vida social e, é claro, o que se espera de um soldado. Todos os elementos desejáveis para integração moral do futuro cidadão: severo mas simples e honesto, trabalhador, obediente às leis, que renega o conforto e o vício. O próprio retrato de Perseus refletido em cada menino grego.\nAtravessando o Mar Jônico até Roma, suas primeiras escolas, cerca de 300 a.C., eram mais restritas que as atenienses, e tinham o propósito de intruir os jovens para a carreira política, sendo compostas por uma pequena e seleta parcela que tinha acesso à educação. Com a ascensão do império e a queda da grécia no século 2 a.C., o grande fluxo de gregos escolarizados para Roma causou o processo de helenização da cidade, de forma que as escolas romanas eram, de fato, escolas gregas ligeiramente modificadas para se adaptar a Roma. Em adição a gramática, composição, ética, história, mitologia e geografia, as escolas de retórica foram desenvolvidas para preparar os profissionais da lei e da vida pública em Roma. Homero continuava a ser o autor favorito em grego, mas agora as escolas contavam também com autores latino-romanos, como Virgílio e Horácio. Nas ciências, um pouco de geometria e astronomia foram adicionadas por sua utilidade prática. Com isso, as sete artes liberais da idade média – gramática, retórica, dialética, aritmética, geometria, música e astronomia – já estavam presentes (Cubberley 1920).\nAssim como na sociedade ateniense, a educação era privada e reservada àqueles que pudessem pagar por ela. Os professores eram, ou pagãos, ou indiferentes a religião; e, por conta disso, as escolas eram cada vez menos frequentadas pelos cristãos, que cresciam rapidamente na população do império. Ao século V, as escolas romanas entraram em declínio, desaparecendo no século seguinte (Williams 2016).\nSegundo Williams (2016), a intensa instabilidade da europa após a queda de Roma, somada à disseminação do cristianismo, tiveram grande impacto na educação, caracterizando o período da alta idade média como de densa ignorância, não apenas entre a população geral, mas também entre os nobres e apenas “levemente mitigada” entre a maior parte do clero. Ele lista como causas para a prevalência da ignorância: 1) a rejeição dos primeiros cristãos em relação a seus opressores pagãos, aí incluindo sua literatura, por conta não apenas de sua origem, como também pela mitologia que carrega; 2) Fora a bíblia, os livros eram caros e raros em relação ao império, uma vez que eles eram manuscritos e copiados à mão por escravos; 3) Esses poucos e caros livros eram escritos em latim, sendo ininteligíveis para a vasta maioria da população, uma vez que os diversos dialetos que eclodiram na região a partir das invasões bárbaras não eram desenvolvidos ou predominantes o suficiente durante a alta idade média; 4) A própria ideia e tradição de educação formal foi perdida culturalmente, perdendo seu valor como algo necessário; 5) Por fim, a partir do século IX, com a expansão do sistema feudal, o isolamento e os perigos associados às viagens durante o período potencializaram o custo de se obter educação e realizar as interações sociais necessárias para o desenvolvimento intelectual.\nA educação formal no início da igreja cristã era apenas catecumenal, e sua principal preocuparação era a regeneração moral da sociedade através da regeneração moral dos convertidos (Williams 2016). Além disso, o esforço educacional da igreja estava fechada nela mesma, com o objetivo de criar sua base teológica, e não voltada para formação intelectual da sociedade a qual servia:\nContrariamente às escolas clássicas, a educação cristã primitiva não possuía vocação intelectual, mas seu apelo era moral e emocional. De fato, os modelos grego e romano eram inteiramente rejeitados – afinal, a educação intelectual pagã era a única disponível e os pais não desejavam que seus filhos tivessem contato e acabassem admirando as deidades do Olímpo. É apenas em meados do século II, com a fundação da escola catequética de Alexandria, que os membros do clero começam a receber treinamento a partir da educação e filosofia grega, formalizando sistematicamente a fé e doutrina cristã, que passa a receber cada vez mais influência do pensamento e filosofia grega. Entretanto, tal movimento seria revertido gradativamente até o início do século V, quando o concílio de Cartago, sob influência de Santo Agostinho, proíbe definitivamente a leitura de autores pagãos pelo clero (Cubberley 1920).\nNo século VI, a partir da fundação do monastério de Montecassino por São Benedito em 529 e a promulgação da regra Beneditina no ano 529, os monastérios se tornam os centros de educação, abertos não apenas para os meninos dispostos a assumir seus votos, como também, posteriormente (séc. IX), para alunos externos sem intenção de tomar votos. As escolas monásticas ofereciam instrução em leitura e escrita (em latim), música, doutrina cristã e regras de conduta. A cópia de manuscritos e preservação de livros antigos era uma das principais atividades dos monges, e, dentre os cristãos, a preservação da literatura clássica foi em grande parte devido a seus esforços (Williams 2016).\nFora do mundo católico romano, a conservação da literatura clássica na Europa em parte se deu por conta dos esforços da Espanha sarracena, no oeste, e dos bizantinos, no leste. Dentre os maometanos – aqui expandindo a visão para califados como Bagdá, Bucara e Damasco –, a educação iniciava pela alfabetização e estudo do Quran, com professores custeados pelo califado. Para as famílias ricas, a educação continuava com o ensino de lógica, filosofia, teologia, astronomia e medicina. Diferente dos povos europeus da alta idade média, dependentes do latim, os sarracenos contavam com traduções em sírio da ciência grega e da filosofia aristotélica, aí incluindo a matemática de Euclides e a álgebra de Diophantus. Eles também foram responsáveis pela criação da química como ciência, além de grandes avanços na álgebra (Williams 2016).\nNos séculos seguintes, ao longo da Baixa Idade Média, o sistema educacional medieval desenvolvido pela igreja se torna mais organizado, fundamentados no Trivium e Quadrivium. Entretanto, a educação continuava voltada para dentro da igreja, com a teologia como a única profissão e carreira a ser obtida através dela.\nNesse sentido, a educação cristã medieval era, em sua essência, um instrumento de reprodução e perpetuação da própria estrutura de poder da igreja. De manutenção da ordem e da hierarquia social, e não de formação intelectual e crítica dos indivíduos.\nEm 1450, o movimento de resgatar a literatura clássica e a filosofia grega e romana, conhecido como Renascimento, começa a se espalhar pela Europa a partir da Itália, trazendo consigo a redescoberta da matemática grega e árabe, além de agregar o estudo das humanidades ao lado da educação moral e física. Esse período marca a ruptura com o pensamento e educação clériga, e lança as bases para a educação moderna. Entretanto, de forma e conteúdo, pouco se tem de diferente da educação clássica grega e romana. Com o apoio das classes dominantes, a ascenção das escolas secundárias e unviersidades renascentistas restauraram a educação do tempo de Cícero, que trazia uma educação aristocrática, em preparação para serviços na Igreja, no Estado e nos grandes negócios.\nHá vários outros eventos na história que podem ser analisados sob a ótica do seu impacto na educação: a reforma protestante1, que, dentre vários outros desdobramentos, gera um novo nível de tolerância religiosa ao conhecimento que acaba por abrir caminho para o surgimento do método científico moderno (Cubberley 1920); o sucesso da contra-reforma e da educação jesuíta nos processos colonizadores, que “promoveram o controle da fé e da moral dos habitantes” (Rosário e Melo 2015); o puritanismo na América do Norte, que estabelece as bases do sistema educacional americano. Entretanto, o propósito deste trabalho não é de forma alguma ser exaustivo, mas sim de argumentar um ponto.\nVoltando à pergunta inicial: o que sobra de puro na educação ao se retirar as relações sociais de exploracão e poder? Este estudante, ao inciar a leitura para este capítulo, tinha como hipótese e espera encontrar evidências de que a educação pré-capitalista tinha em seu cerne a curiosidade e o espírito humano, com foco nas artes. Entretanto, a história nos mostra que o elemento em comum que aparecem em todas as sociedades é a normalização moral do indivíduo e de reprodução das relações sociais vigentes, através da transmissão de valores e normas sociais e da formação dos filhos das classes dominantes para ocupar determinados postos na sociedade."
  },
  {
    "objectID": "posts/202208_educacao-pre-capitalista/index.html#footnotes",
    "href": "posts/202208_educacao-pre-capitalista/index.html#footnotes",
    "title": "O Papel Da Educação Matemática Sob A Hegemonia Do Capital",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nCalvino escreve para o Príncipe de Genebra, em 1541, que “as artes liberais e a boa educação são bons auxiliares no pleno conhecimento da Palavra”.↩︎"
  },
  {
    "objectID": "posts/202504_all-models-are-wrong/index.html",
    "href": "posts/202504_all-models-are-wrong/index.html",
    "title": "Todos os modelos estão errados, mas alguns são inúteis",
    "section": "",
    "text": "TL;DR\n\n\n\nQuando usar o coeficiente de correlação de Pearson, sempre teste a significância. Mas não o utilize para analisar séries temporais. É um erro comum. Isso viola a independência das observações e ignora o relacionamento entre defasagens. Em vez disso, use a análise de cross-correlograma para identificar relações entre séries temporais, incluindo relações defasadas.\n“Todos os modelos estão errados, mas alguns são úteis” é uma frase recorrente entre aqueles que praticam estatística. Ela se origina de uma afirmação de George Box, um dos grandes estatísticos do século XX: “Since all models are wrong the scientist must be alert to what is importantly wrong.” (Box 1976). “Todos os modelos estão errados” significa que os modelos são intrinsecamente limitados e não capturam perfeitamente a realidade. Em outras palavras, um modelo é uma representação simplificada da realidade, usada para explicar ou prever um fenômeno. Se fosse uma explicação perfeita desse fenômeno, deixaria de ser um modelo e se tornaria uma lei.\nNa estatística, lidamos essencialmente com variáveis aleatórias ou estocásticas, ou seja, variáveis que possuem uma distribuição de probabilidade (Gujarati e Porter 2021). Nossa missão como analistas é desenvolver e utilizar métodos que nos digam como formular funções que nos permitam descrever e prever o relacionamento entre essas variáveis, minimizando os erros estocásticos.\nDependendo da forma funcional e do método de estimação escolhido, há uma série de pressupostos que devem ser atendidos para que qualquer inferência sobre o erro, coeficientes, preditores e regressandos seja válida. Se esses pressupostos forem ignorados, não há garantia de que os resultados encontrados sejam uma aproximação ótima da função que se deseja estimar. Mais do que isso, a violação de alguns desses pressupostos pode gerar resultados enganosos, mostrando relações estatísticas significativas onde não deveria haver, subestimando ou superestimando o objeto de estudo.\nNeste post, abordo alguns erros metodológicos frequentes que fazem com que alguns modelos sejam inúteis."
  },
  {
    "objectID": "posts/202504_all-models-are-wrong/index.html#footnotes",
    "href": "posts/202504_all-models-are-wrong/index.html#footnotes",
    "title": "Todos os modelos estão errados, mas alguns são inúteis",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUm resumo pode ser encontrado em Cambridge University (2021).↩︎"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html",
    "href": "posts/202101_r-vscode/index.html",
    "title": "R em 2021 com o VSCode",
    "section": "",
    "text": "Comecei a usar o VSCode em outubro de 2020 quando comecei a estudar Python. Pesquisando sobre o setup ideal, li que a IDE de menor impacto para usuários de R (R Core Team 2020) seria o Spyder, mas que a experiência de um usuário Python nativo seria com o VSCode, então foi pra lá que eu fui, mesmo sendo uma experiência mais complicada.\nRapidamente me apaixonei por sua capacidade e maturidade. Há extensões da comunidade para tudo, o que torna a experiência muito agradável! Entretanto, o ecossistema do R ainda era muito voltado ao RStudio e eu ainda me via preso naquela IDE no meu dia-a-dia com o R. Claro que isso mudou quando vi um tweet do Miles McBain sobre a adição dos RStudio Addins ao VSCode.\nPassando a considerar o VSCode como uma possibilidade real para R, dei uma pesquisada e achei esse post do Kun Ren que oferece um setup para trabalhar em R no VSCode. Neste post falo das funcionalidades e de como configurar."
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#intellisense",
    "href": "posts/202101_r-vscode/index.html#intellisense",
    "title": "R em 2021 com o VSCode",
    "section": "1.1 Intellisense",
    "text": "1.1 Intellisense\nEsse ouro do VSCode te permite ver a documentação da função (quick info), ou do dataset, e de todos seus argumentos (parameter info) apenas passando o mouse em cima. Além disso, é a funcionalidade que adiciona code completion e member list, dentre outros.\n\n\n\ncode completion, hover, quick info, parameter info\n\n\n\n\n\nseleção de cores via IDE (｡◕‿◕｡)"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#integração-com-git-e-github",
    "href": "posts/202101_r-vscode/index.html#integração-com-git-e-github",
    "title": "R em 2021 com o VSCode",
    "section": "1.2 Integração com Git e GitHub",
    "text": "1.2 Integração com Git e GitHub\nAs extensões GitHub Pull Requests and Issues e Git Lens permitem uma integração profunda com o GitHub, de forma que você não precisa deixar o VSCode. Você pode abrir, comentar e fechar issues e PRs; enviar, visualizar e editar commits; realizar quaisquer comandos bash via command pallet (push, pull, checkout, prune, rebase etc), dentre outros.\n\n\n\ncriando issue\n\n\nClicando na issue, o VSCode já abre um novo branch e faz o checkout para você trabalhar nela (e ainda deixa engatilhada uma commit message personalizada!).\n\n\n\ncriando um novo branch via issue\n\n\nE, ao final do trabalho, basta clicar em criar um novo PR que a própria IDE envia o branch pra origem e traz a interface do PR. Tudo sem abrir o navegador ou o terminal.\n\n\n\ncriando pull request"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#múltiplos-terminais",
    "href": "posts/202101_r-vscode/index.html#múltiplos-terminais",
    "title": "R em 2021 com o VSCode",
    "section": "1.3 Múltiplos Terminais",
    "text": "1.3 Múltiplos Terminais\nEnquanto seu blog ou Shiny app está renderizando e, consequentemente, ocupando um terminal, você pode simplesmente abrir outro e continuar trabalhando normalmente!\n\n\n\npode-se abrir quantos terminais quiser!"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#draw.io",
    "href": "posts/202101_r-vscode/index.html#draw.io",
    "title": "R em 2021 com o VSCode",
    "section": "1.4 Draw.io",
    "text": "1.4 Draw.io\nEste é um exemplo de uma das inúmeras extensões úteis que a comunidade disponibiliza no VSCode. A extensão draw.io integra o diagrams.net ao VSCode. Com ela, podemos fazer diagramas de forma muito rápida e sem precisar de sair da IDE!\n\n\n\npara criar um diagrama, basta criar um arquivo com a extensão .drawio\n\n\n\n\n\ndiagrama que fiz para o esse post usando a extensão"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#live-share",
    "href": "posts/202101_r-vscode/index.html#live-share",
    "title": "R em 2021 com o VSCode",
    "section": "1.5 Live Share",
    "text": "1.5 Live Share\nJá sonhou em trabalhar no mesmo script com sua galera ao vivasso? A extensão Live Share permite isso, além de fornecer canais de chat e de áudio, o que torna desnecessário abrir uma chamada de áudio em outro app enquanto trabalha!\n\n\n\npraticamente todo mundo agora\n\n\nAlém disso, você não precisa ter o interpretador da linguagem instalado para participar da sessão. Isso significa que mesmo que você esteja em outra máquina, que não tenha o R ou Python (ou qualquer outra) instalados, você pode entrar na sessão e colaborar nos scripts de seus colegas, inclusive usando os terminais deles para executar o código!\n\n\n\nvisão de duas máquinas visualizando uma a outra durante uma sessão de Live Share no VSCode"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#windows-subsystem-for-linux",
    "href": "posts/202101_r-vscode/index.html#windows-subsystem-for-linux",
    "title": "R em 2021 com o VSCode",
    "section": "1.6 Windows Subsystem for Linux",
    "text": "1.6 Windows Subsystem for Linux\nEstá em uma máquina Windows e já precisou de depurar alguma treta em ambiente Linux? Então você já teve o desprazer de instalar máquinas virtuais ou dual boot (╯°□°）╯︵ ┻━┻\nBoa notícia: com a extensão Remote - WSL sua pasta (projeto) é copiada e reaberta em um ambiente Linux, com o terminal bonitinho e pronto para o uso. Tudo isso a um clique de distância!\n\n\n\niniciando uma sessão em ambiente Linux"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#linter-integrado",
    "href": "posts/202101_r-vscode/index.html#linter-integrado",
    "title": "R em 2021 com o VSCode",
    "section": "1.7 Linter integrado",
    "text": "1.7 Linter integrado\nA extensão do R no VSCode integra o pacote {lintr} à IDE, de forma que você tem as informações de ajustes de estilo a se fazer em seu código em tempo real.\n\n\n\nverificação de estilo em tempo real"
  },
  {
    "objectID": "posts/202101_r-vscode/index.html#footnotes",
    "href": "posts/202101_r-vscode/index.html#footnotes",
    "title": "R em 2021 com o VSCode",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nagora integrado na extensão vscode-R.↩︎"
  },
  {
    "objectID": "posts/202210_series-hierarquicas/index.html",
    "href": "posts/202210_series-hierarquicas/index.html",
    "title": "Séries Temporais Hierárquicas: teoria",
    "section": "",
    "text": "Já faz quase uma década que chega nessa época do ano e eu trabalho com previsões de séries temporais lá no Banestes. Na hora de estabelecer metas e objetivos orçamentários (saldos, receitas, despesas etc), saber modelar e fazer previsões é uma mão na roda.\nClaro que sozinhas não devem ser adotadas como finais ou verdadeiras, uma vez que não usam da informação gerencial e expectativas dos stakeholders, por exemplo. Mas são um ponto de partida razoável que serve como base para elaborar e construir expectativas, plano de ação etc."
  },
  {
    "objectID": "posts/202210_series-hierarquicas/index.html#séries-hierárquicas-e-séries-agrupadas",
    "href": "posts/202210_series-hierarquicas/index.html#séries-hierárquicas-e-séries-agrupadas",
    "title": "Séries Temporais Hierárquicas: teoria",
    "section": "SÉRIES HIERÁRQUICAS E SÉRIES AGRUPADAS",
    "text": "SÉRIES HIERÁRQUICAS E SÉRIES AGRUPADAS\nSéries temporais hierárquicas são aquelas que podem ser agregadas ou desagregadas naturalmente em uma estrutura aninhada (R. J. Hyndman e Athanasopoulos 2021). Para ilustrar, tome a série do PIB brasileiro. Ela pode ser desagregada por estado que, por sua vez, pode ser desagregada por município.\n\n\n\n\n\n\n\n\nFigura 1: Séries Hierárquicas.\n\n\n\n\n\nEssa estrutura pode ser representada por equações para qualquer nível de agregação.\n\\[\n\\begin{align}\ny_t &= y_{A,t} + y_{B,t} + y_{C,t} \\\\\ny_t &= y_{AA,t} + y_{AB,t} + y_{AC,t} + y_{BA,t} + y_{BC,t} + y_{CA,t} \\\\\ny_{A,t} &= y_{AA,t} + y_{AB,t} + y_{AC,t}\n\\end{align}\n\\]\nAssim, o agregado nacional pode ser representado apenas pelos agregados dos estados, através de (1), ou como o agregado dos municípios (2). Já o agregado para o estado do Espírito Santo é representado por (3).\nAlternativamente, podemos descrever a estrutura completa de forma matricial:\n\\[\n\\begin{bmatrix}\n    y_{t} \\\\\n    y_{A, t} \\\\\n    y_{B, t} \\\\\n    y_{C, t} \\\\\n    y_{AA, t} \\\\\n    y_{AB, t} \\\\\n    y_{AC, t} \\\\\n    y_{BA, t} \\\\\n    y_{BB, t} \\\\\n    y_{BC, t} \\\\\n    y_{CA, t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n    1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    y_{AA, t} \\\\\n    y_{AB, t} \\\\\n    y_{AC, t} \\\\\n    y_{BA, t} \\\\\n    y_{BB, t} \\\\\n    y_{BC, t} \\\\\n    y_{CA, t}\n\\end{bmatrix}\n\\]\nPor outro lado, o PIB pode ser também desagregado de forma cruzada de acordo com a atividade econômica — lavoura, rebanho, indústria de transformação, extrativa, bens de capital, bens intermediários, comércio de vestuário, automotivos, serviços etc. Essa estrutura não pode ser desagregada naturalmente de uma única forma, como é a hierarquia de estados e municípios. Não pode ser aninhada por um atributo como a própria geografia. A esse tipo de estrutura dá-se o nome de séries agrupadas.\n\n\n\n\n\n\n\n\nFigura 2: Séries agrupadas.\n\n\n\n\n\nCombinando as duas, temos a estrutura de séries hierárquicas agrupadas. Ao contrário da estrutura hierárquica, que só pode ser agregada de uma forma — como com os municípios abaixo dos estados —, a adição da estrutura agrupada pode ocorrer tanto acima (Figura 3) quanto abaixo (Figura 4) da hierárquica.\n\n\n\n\n\n\n\n\nFigura 3: Séries Hierárquicas Agrupadas (a).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 4: Séries Hierárquicas Agrupadas (b).\n\n\n\n\n\nNa notação matricial, a estrutura da Figura 4 é representada como abaixo. Formalmente, o primeiro membro da igualdade é composto pelo vetor \\(\\mathbf{y}_t\\) \\(n\\)-dimensional com todas as observações no tempo \\(t\\) para todos os níveis da hierarquia. O segundo membro é composto pela matriz de soma \\(\\mathbf{S}\\) de dimensão \\(n \\times m\\) que define as equações para todo nível de agregação, e pela matriz \\(\\mathbf{b}_t\\) composta pelas séries no nível mais desagregado.\n\\[\\begin{equation}\n\\mathbf{y}_t=\\mathbf{Sb}_t\n\\end{equation}\\]\n\\[\\begin{equation}\n\\begin{bmatrix}\n    y_{t} \\\\\n    y_{A, t} \\\\\n    y_{B, t} \\\\\n    y_{C, t} \\\\\n    y_{X, t} \\\\\n    y_{Y, t} \\\\\n    y_{Z, t} \\\\\n    y_{AX, t} \\\\\n    y_{AY, t} \\\\\n    y_{AZ, t} \\\\\n    y_{BX, t} \\\\\n    y_{BY, t} \\\\\n    y_{BZ, t} \\\\\n    y_{CX, t} \\\\\n    y_{CY, t} \\\\\n    y_{CZ, t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\\\\n    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\\\\n    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n    y_{AX, t} \\\\\n    y_{AY, t} \\\\\n    y_{AZ, t} \\\\\n    y_{BX, t} \\\\\n    y_{BY, t} \\\\\n    y_{BZ, t} \\\\\n    y_{CX, t} \\\\\n    y_{CY, t} \\\\\n    y_{CZ, t}\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/202210_series-hierarquicas/index.html#abordagens-top-down-e-bottom-up",
    "href": "posts/202210_series-hierarquicas/index.html#abordagens-top-down-e-bottom-up",
    "title": "Séries Temporais Hierárquicas: teoria",
    "section": "ABORDAGENS TOP-DOWN E BOTTOM-UP",
    "text": "ABORDAGENS TOP-DOWN E BOTTOM-UP\nTalvez as formas mais intuitivas de se pensar em previsões para esses tipos de estrutura sejam as abordagens top-down e bottom-up. Tome a estrutura descrita na Figura @ref(fig:h), por exemplo. Podemos realizar a previsão para o horizonte de tempo \\(h\\) do agregado do PIB brasileiro, representado no topo da hierarquia por Total (6), e então distribuir os valores previstos proporcionalmente entre os estados e municípios.\n\\[\\begin{equation}\n\\hat{y}_{T+h | T} = E[y_{T+h} | \\Omega_T]\n\\end{equation}\\]\nEssa é a abordagem top-down. Nela, a previsão para os níveis mais desagregados da hierarquia são determinadas por uma proporção \\(p_i\\) do nível agregado. Por exemplo, as previsões para Vitória são dadas pela equação (7).\n\\[\\begin{equation}\n\\tilde{y}_{AA, T+h | T} = p_{1}\\hat{y}_{T+h | T}\n\\end{equation}\\]\nPara isso, temos de definir uma matriz com todos esses pesos, que, seguindo a formulação de R. J. Hyndman e Athanasopoulos (2021), vamos chamar de \\(\\mathbf{G}\\):\n\\[\\begin{equation}\n\\mathbf{G}\n=\n\\begin{bmatrix}\n    p_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{equation}\\]\n\\(\\mathbf{G}\\) é uma matriz \\(m \\times n\\) que multiplica a matriz \\(\\hat{\\mathbf{y}}_{T+h|T}\\) que, por sua vez, é composta pelas previsões base — as previsões individuais para todos os níveis de agregação. A equação para a abordagem top-down será, então:\n\\[\\begin{equation}\n\\mathbf{\\tilde{y}}_{T+h | T} = \\mathbf{SG\\hat{y}}_{T+h | T}\n\\end{equation}\\]\nNa notação matricial para a estrutura da Figura 1, temos:\n\\[\\begin{equation}\n\\begin{bmatrix}\n    \\tilde{y}_{t} \\\\\n    \\tilde{y}_{A, t} \\\\\n    \\tilde{y}_{B, t} \\\\\n    \\tilde{y}_{C, t} \\\\\n    \\tilde{y}_{AA, t} \\\\\n    \\tilde{y}_{AB, t} \\\\\n    \\tilde{y}_{AC, t} \\\\\n    \\tilde{y}_{BA, t} \\\\\n    \\tilde{y}_{BB, t} \\\\\n    \\tilde{y}_{BC, t} \\\\\n    \\tilde{y}_{CA, t}\n\\end{bmatrix}\n=\n\\mathbf{S}\n\\begin{bmatrix}\n    p_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    p_7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\hat{y}_{T+h|T} \\\\\n    \\hat{y}_{A, T+h|T} \\\\\n    \\hat{y}_{B, T+h|T} \\\\\n    \\hat{y}_{C, T+h|T} \\\\\n    \\hat{y}_{AA, T+h|T} \\\\\n    \\hat{y}_{AB, T+h|T} \\\\\n    \\hat{y}_{AC, T+h|T} \\\\\n    \\hat{y}_{BA, T+h|T} \\\\\n    \\hat{y}_{BB, T+h|T} \\\\\n    \\hat{y}_{BC, T+h|T} \\\\\n    \\hat{y}_{CA, T+h|T}\n\\end{bmatrix}\n\\end{equation}\\]\nO que nos dá uma proporção do total para cada elemento no nível mais desagregado. \\[\\begin{equation}\n\\begin{bmatrix}\n    \\tilde{y}_{t} \\\\\n    \\tilde{y}_{A, t} \\\\\n    \\tilde{y}_{B, t} \\\\\n    \\tilde{y}_{C, t} \\\\\n    \\tilde{y}_{AA, t} \\\\\n    \\tilde{y}_{AB, t} \\\\\n    \\tilde{y}_{AC, t} \\\\\n    \\tilde{y}_{BA, t} \\\\\n    \\tilde{y}_{BB, t} \\\\\n    \\tilde{y}_{BC, t} \\\\\n    \\tilde{y}_{CA, t}\n\\end{bmatrix}\n=\n\\mathbf{S}\n\\begin{bmatrix}\n    p_1\\hat{y}_{T+h|T} \\\\\n    p_2\\hat{y}_{T+h|T} \\\\\n    p_3\\hat{y}_{T+h|T} \\\\\n    p_4\\hat{y}_{T+h|T} \\\\\n    p_5\\hat{y}_{T+h|T} \\\\\n    p_6\\hat{y}_{T+h|T} \\\\\n    p_7\\hat{y}_{T+h|T}\n\\end{bmatrix}\n\\end{equation}\\]\nSubstituindo a matriz \\(\\mathbf{S}\\), temos as equações que definem cada previsão da estrutura em função de proporções da previsão do agregado.\n\\[\\begin{equation}\n\\begin{bmatrix}\n    \\tilde{y}_{t} \\\\\n    \\tilde{y}_{A, t} \\\\\n    \\tilde{y}_{B, t} \\\\\n    \\tilde{y}_{C, t} \\\\\n    \\tilde{y}_{AA, t} \\\\\n    \\tilde{y}_{AB, t} \\\\\n    \\tilde{y}_{AC, t} \\\\\n    \\tilde{y}_{BA, t} \\\\\n    \\tilde{y}_{BB, t} \\\\\n    \\tilde{y}_{BC, t} \\\\\n    \\tilde{y}_{CA, t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n    1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n    p_1\\hat{y}_{T+h|T} \\\\\n    p_2\\hat{y}_{T+h|T} \\\\\n    p_3\\hat{y}_{T+h|T} \\\\\n    p_4\\hat{y}_{T+h|T} \\\\\n    p_5\\hat{y}_{T+h|T} \\\\\n    p_6\\hat{y}_{T+h|T} \\\\\n    p_7\\hat{y}_{T+h|T}\n\\end{bmatrix}\n\\end{equation}\\]\nJá a abordagem bottom-up parte do raciocínio inverso e define as previsões de cada elemento da estrutura a partir das previsões dos elementos mais desagregados. Para tanto, basta modificar a matriz \\(\\mathbf{G}\\).\n\\[\\begin{equation}\n\\mathbf{G}\n=\n\\begin{bmatrix}\n    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\end{equation}\\]\nO que resulta nas equações desejadas. Portanto, \\(\\mathbf{G}\\) define a abordagem — se top-down ou bottom-up —, e \\(\\mathbf{S}\\) define a maneira da qual as previsões são somadas para formar as equações de previsão para cada elemento da estrutura.\n\\[\\begin{equation}\n\\begin{bmatrix}\n    \\tilde{y}_{t} \\\\\n    \\tilde{y}_{A, t} \\\\\n    \\tilde{y}_{B, t} \\\\\n    \\tilde{y}_{C, t} \\\\\n    \\tilde{y}_{AA, t} \\\\\n    \\tilde{y}_{AB, t} \\\\\n    \\tilde{y}_{AC, t} \\\\\n    \\tilde{y}_{BA, t} \\\\\n    \\tilde{y}_{BB, t} \\\\\n    \\tilde{y}_{BC, t} \\\\\n    \\tilde{y}_{CA, t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n    1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\hat{y}_{AA, T+h|T} \\\\\n    \\hat{y}_{AB, T+h|T} \\\\\n    \\hat{y}_{AC, T+h|T} \\\\\n    \\hat{y}_{BA, T+h|T} \\\\\n    \\hat{y}_{BB, T+h|T} \\\\\n    \\hat{y}_{BC, T+h|T} \\\\\n    \\hat{y}_{CA, T+h|T}\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/202210_series-hierarquicas/index.html#coerência-e-reconciliação",
    "href": "posts/202210_series-hierarquicas/index.html#coerência-e-reconciliação",
    "title": "Séries Temporais Hierárquicas: teoria",
    "section": "COERÊNCIA E RECONCILIAÇÃO",
    "text": "COERÊNCIA E RECONCILIAÇÃO\nSeja somando as previsões do nível mais desagregado para formar os níveis superiores da hierarquia (bottom-up) ou distribuindo proporcionalmente as previsões do nível mais agregado (top-down), o vetor \\(\\mathbf{\\tilde{y}}_t\\) representa as previsões coerentes. Isso significa que as previsões “batem”, ou seja, são totalizadas corretamente — as previsões de cada elemento agregado corresponde ao somatório das previsões dos níveis inferiores da hierarquia. Isso é garantido pela multiplicação das matrizes \\(\\mathbf{SG}\\).\nNão fosse essa pré multiplicação, nada garantiria a coerência das previsões. Tomando a estrutura da Figura 1 como exemplo, seria um acaso improvável que as previsões do agregado para o estado do Espírito Santo fossem exatamente a soma das previsões individuais de seus municípios. Isso porque cada série pode seguir um processo diferente (e.g., arima) com erros e variâncias distintas.\nOs métodos de gerar previsões coerentes a partir de previsões base são chamados de métodos de reconciliação. Os métodos de reconciliação tradicionais apresentados, top-down e bottom-up, utilizam informação limitada. No método top-down, utiliza-se apenas informações do nível mais agregado — por isso, apenas a primeira coluna em (8) é diferente de zero. Já na abordagem bottom-up, utiliza-se apenas as informações dos níveis mais desagregados, o que resulta na submatriz identidade \\(m \\times m\\) em (13), enquanto as colunas que representam os níveis mais agregados são nulas.\nAlternativamente, podemos pensar numa matriz \\(\\mathbf{G}\\) qualquer que utilize toda a informação disponível e tenha algumas propriedades que garantam que as previsões coerentes tenham o menor erro o possível. Esse é o problema de pesquisa trabalhado na reconciliação ótima, e é um assunto relativamente novo. Quer dizer, previsões pontuais de séries temporais hierárquicas é um troço antigo. Ao menos desde a década de 70, pesquisas foram publicadas acerca de abordagens bottom-up e top-down, suas vantagens e desvantagens, e tentativas de se definir qual é o melhor método1. Entretanto, é apenas em R. J. Hyndman et al. (2011) que é formalizada uma abordagem prática que utiliza toda a informação disponível, (i.e. as previsões de todos elementos de todos os níveis da hierarquia) a partir da estimação da matriz \\(\\mathbf{G}\\) via regressão linear por mínimos quadrados generalizados (MQG).\nEntretanto, para ser capaz de estimar o modelo por MQG, é necessária a matriz de variância-covariância dos erros. R. J. Hyndman et al. (2011) usam a matriz de erros de coerência, ou seja, a diferença entre as previsões reconciliadas e as previsões base, que tem posto incompleto e não identificada e, portanto, não pode ser estimada. Os autores contornam esse problema adotando no lugar da matriz de variância-covariância dos erros uma matriz diagonal constante, ou seja, assumem variância constante dos erros de reconciliação, o que acaba caindo na estimação de \\(\\mathbf{G}\\) por mínimos quadrados ordinários (MQO).\nA estimação por esse método resulta numa reconciliação ótima que depende apenas da matriz \\(\\mathbf{S}\\), ou seja, da estrutura hierárquica, e independe da variância e covariância das previsões base \\(\\mathbf{\\hat{y}_{T+h}}\\) — o que não é uma conclusão satisfatória.\nRob J. Hyndman, Lee, e Wang (2016) tentam aperfeiçoar o método usando as variâncias das previsões base estimadas (dentro da amostra) como estimativa para a matriz de variância-covariância dos erros de reconciliação, de forma a as utilizar como pesos e realizar a reconciliação ótima por mínimos quadrados ponderados (MQP). Assim, previsões base mais acuradas têm peso maior do que as mais ruidosas. Entretanto, não fornecem justificativa teórica para usar a diagonal da matriz de variância-covariância de \\(\\mathbf{\\hat{e}_{t}}\\).\nWickramasuriya, Athanasopoulos, e Hyndman (2019) argumentam que o que de fato interessa é que as previsões reconciliadas tenham o menor erro — não interessa que a previsão reconciliada seja mais próxima da previsão base, mas que as reconciliadas sejam as mais precisas o possível, já que é a que se vai utilizar!. Então, corrigem a abordagem de reconciliação ótima para o objetivo de minimização dos erros das previsões reconciliadas \\(\\mathbf{\\tilde{e}_{t+h}} = \\mathbf{y_{t+h} - \\mathbf{\\tilde{y}_{t+h}}}\\), ao invés dos erros de reconciliação das previsões base \\(\\mathbf{\\hat{y}_{t+h}} - \\mathbf{\\tilde{y}_{t+h}}\\). Dado que isso implica na minimização da variância de \\(\\mathbf{\\tilde{e}_{t+h}}\\), ou seja, na minimização do somatório da diagonal, o traço, da matriz de variância-covariância de \\(\\mathbf{\\tilde{e}_{t+h}}\\), eles chamaram esse método de Menor Traço (MinT, na sigla em inglês). Paralelamente, usam desigualdade triangular para demonstrar que as previsões reconciliadas obtidas por esse método são ao menos tão boas quanto as previsões base.\nE, de 2021 pra cá, há desenvolvimentos interessantes na extensão probabilística desse corpo teórico (e é onde estou realizando minha pesquisa de mestrado). Panagiotelis et al. (2021) reinterpreta a literatura de coerência e reconciliação de previsões pontuais a partir de uma abordagem geométrica, trazendo provas alternativas para conclusões anteriores ao mesmo tempo em que fornece novos teoremas. Além disso, Panagiotelis et al. (2021) estende essa interpretação geométrica para o contexto probabilístico, fornecendo métodos paramétricos e não paramétricos (via bootstrapping) para reconciliação de previsões probabilísticas, ou seja, para reconciliar previsões \\(\\hat{y}_t\\) obtidas a partir de toda a distribuição, e não apenas a média."
  },
  {
    "objectID": "posts/202210_series-hierarquicas/index.html#footnotes",
    "href": "posts/202210_series-hierarquicas/index.html#footnotes",
    "title": "Séries Temporais Hierárquicas: teoria",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUma revisão dessa literatura pode ser encontrada em Athanasopoulos, Ahmed, e Hyndman (2009).↩︎"
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "",
    "text": "Ainda há um bocado de coisas para cobrir nessa série de “inferência 101” e, para continuar os estudos, trouxe a base da RAIS (Relação Anual de Informações Sociais) de 20171. Com ela, vamos introduzir o conceito de effect size e de quebra analisar a desigualdade de gênero em nossa querida capital Vitória."
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#verificando-a-hipótese-da-normalidade",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#verificando-a-hipótese-da-normalidade",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "2.1 VERIFICANDO A HIPÓTESE DA NORMALIDADE",
    "text": "2.1 VERIFICANDO A HIPÓTESE DA NORMALIDADE\nSabemos que o teste t é um teste paramétrico e já discutimos anteriormente que, quando os dados não seguem uma distribuição próxima da normal, precisamos realizar transformações para alcançar a normalidade.\nVocê pode pensar que, com uma amostra desse tamanho, o Teorema Central do Limite garante a hipótese da normalidade. Entretanto, principalmente em dados com muitos outliers extremos, o tamanho da amostra necessária para convergência pode ser absurda, invalidando, na prática, a afirmação a partir da TCL.3. Portanto, defendo que devemos, sim, ter cuidado com essa hipótese mesmo lidando com grandes amostras.\nA primeira análise nesse sentido é a visual. Podemos verificar que a distribuição apresenta fat tail e passa longe de uma normal, tanto pelo histograma quanto pelo Q-Q plot.\n\n# histograma\ndata %&gt;%\n  ggplot(aes(\n    x = rem_media,\n    fill = factor(sexo)\n  )) +\n  geom_histogram(binwidth = 500) +\n  coord_cartesian(xlim = c(0, 20000)) +\n  scale_y_continuous(labels = scales::number) +\n  scale_x_continuous(labels = scales::number) +\n  scale_fill_manual(\n    name = \"sexo\",\n    labels = c(\"homens\", \"mulheres\"),\n    values = c(\"lightblue\", \"salmon\")\n  ) +\n  labs(\n    x = \"remuneração média\",\n    y = \"quantidade\",\n    title = \"DISTRIBUIÇÃO DA REMUNERAÇÃO MÉDIA ENTRE HOMENS E MULHERES\",\n    subtitle = \"amostra da cidade de Vitória-ES\",\n    caption = \"fonte: Rais/2017\"\n  ) +\n  theme_minimal() +\n  theme(text = element_text(\n    family = \"Century Gothic\",\n    color = \"grey30\"\n  ))\n\n\n\n\n\n\n\n# qqplot\n# os dados estarão em cima da reta caso sejam distribuídos normalmente\npar(mfrow = c(1, 2))\n\nqqnorm(data[data$sexo == 1, ]$rem_media,\n  main = \"Q-Q PLOT: HOMENS\")\nqqline(data[data$sexo == 1, ]$rem_media)\n\nqqnorm(data[data$sexo == 2, ]$rem_media,\n  main = \"Q-Q PLOT: MULHERES\")\nqqline(data[data$sexo == 2, ]$rem_media)\n\n\n\n\n\n\n\n\nPodemos realizar um experimento para verificar a velocidade de convergência para a distribuição normal. Calculando a distribuição de mil médias de 30 homens cada, se ela apresentar distribuição próxima da normal poderemos assumir a normalidade e prosseguir com o trabalho. Caso contrário, precisaremos tratar a base.\n\n# garantir reprodutibilidade\nset.seed(1)\n\n# quantidade de amostras\nn = 1000\n\n# médias\nmedias = rep(NA, n)\n\n# tirando amostras e calculando as médias\nfor (i in 1:n) {\n  medias[i] = mean(\n    sample(data[data$sexo == 1, ]$rem_media,\n      size = 30, replace = TRUE\n    )\n  )\n}\n\n# qqplot\nqqnorm(medias)\nqqline(medias)\n\n\n\n\n\n\n\n# visualização\nhist(medias,\n    main = \"DISTRIBUIÇÃO DAS MÉDIAS AMOSTRAIS\",\n    xlab = \"médias\",\n    sub = \"seed = 1\"\n)\n\n\n\n\n\n\n\n\nUfa! O TCL se manteve em nossos dados e tanto o Q-Q plot quanto o histograma demonstraram distribuição próxima da normal, nos habilitando a prosseguir com os testes."
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#teste-t",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#teste-t",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "2.2 TESTE T",
    "text": "2.2 TESTE T\nAgora que garantimos os pré-requisitos, vamos testar se a diferença entre as médias é significativa. Para isso, vamos usar o pacote {infer}:\n\n# transformando variável `sexo`\ndata = data %&gt;%\n  mutate(sexo = ifelse(sexo == 1, \"homem\", \"mulher\"))\n\n# carregando pacote\nlibrary(infer)\n\n# calculando estatística t\nestatistica_calculada = data %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# gerando distribuição nula\ndistr_nula = data %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 100, type = \"permute\") %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# visualizando distribuição nula e estatística do teste\ndistr_nula %&gt;%\n  visualize(method = \"both\") +\n  shade_p_value(estatistica_calculada, direction = \"greater\")\n\n\n\n\n\n\n\n# calculando p-valor\ndistr_nula %&gt;%\n  get_p_value(obs_stat = estatistica_calculada, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nCom p-valor de 0% e uma estatística calculada a quilômetros da distribuição nula, fica bem claro que a diferença é significativa. Entretanto, isso não quer dizer que ela seja grande ou pequena. É aí que entram os indicadores de tamanho de efeito."
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#tamanho-do-efeito-cohens-d",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#tamanho-do-efeito-cohens-d",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "2.3 TAMANHO DO EFEITO: Cohen’s D",
    "text": "2.3 TAMANHO DO EFEITO: Cohen’s D\nNesse post eu mostrei que o tamanho da amostra é determinante para o teste t e tenderemos a rejeitar a hipótese nula quanto maior for a amostra. Aqui, se tratando de mais de 140 mil observações para cada sexo, dificilmente essas diferenças não seriam significativas. Para complementar esse teste, podemos calcular o tamanho do efeito com a estatística d de Cohen.\nMuito usada no campo da saúde em experimentos com testes t pareados, quando é preciso definir o efeito de uma ação em um grupo de tratamento em relação ao grupo de controle, a estatística d de Cohen é um indicador de diferenças padronizadas e é particularmente valiosa para quantificar o efeito de uma intervenção, seja em políticas públicas ou em ações de marketing na sua empresa. Isso quer dizer que ele enfatiza o tamanho da diferença entre as médias, sem confundir com a questão do tamanho da amostra.\nPor ser padronizado (i.e. não é medido na unidade da amostra, aqui em Reais, mas sim em desvios-padrões) pode ser facilmente comparado à diferentes experimentos. Sua forma de cálculo é a seguinte: \\[ d = \\frac{\\text{média do grupo experimental} - \\text{média do grupo de controle}}{\\text{desvio padrão agrupado}} \\]\nPara duas médias fixadas, quanto menor for o desvio padrão, maior será a estatística d. Por outro lado, quanto maior for o desvio padrão, menor será o tamanho do efeito.\nNo R, vamos utilizar o pacote {effectsize}:\n\n# carregando pacote\nlibrary(effectsize)\n\n# calculando a diferença padronizada\ncohens_d(\n  data[data$sexo == \"homem\", ]$rem_media,\n  data[data$sexo == \"mulher\", ]$rem_media\n)\n\nCohen's d |       95% CI\n------------------------\n0.26      | [0.25, 0.26]\n\n- Estimated using pooled SD.\n\n# interpretando a estatística d\ninterpret_cohens_d(0.26, \"gignac2016\")\n\n[1] \"small\"\n(Rules: gignac2016)\n\n\nCom uma estatística \\(d=0.26\\), a diferença é considerada pequena. Na escala de Gignac & Szodorai (2016):\n\nrules(\n  c(0.2, 0.41, 0.63),\n  c(\"muito pequeno\", \"pequeno\", \"moderado\", \"grande\"),\n  \"Gignac & Szodorai (2016)\"\n)\n\n# Reference Thresholds (Gignac & Szodorai (2016))\n\n           Label            \n----------------------------\n       muito pequeno &lt;=  0.2\n 0.2 &lt;    pequeno    &lt;= 0.41\n0.41 &lt;   moderado    &lt;= 0.63\n0.63 &lt;    grande            \n\n\nEm uma situação de equidade, 50% das mulheres teriam remuneração abaixo do que o homem médio, enquanto a outra metade teria remuneração superior.\nUm efeito de 0.26, equivale a dizer que 62% das mulheres teriam remuneração abaixo da remuneração do homem médio em Vitória, enquanto apenas 38% receberiam mais do que o homem médio4.\nClaro que analisar o agregado significa olhar para a média. Como serão essas diferenças dentre as diversas profissões?"
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#disparidade-salarial-entre-médicos",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#disparidade-salarial-entre-médicos",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "3.1 DISPARIDADE SALARIAL ENTRE MÉDICOS",
    "text": "3.1 DISPARIDADE SALARIAL ENTRE MÉDICOS\nPrimeiramente, vamos verificar a significância da diferença. Como esperado, ela é significativa.\n\n# calculando estatística t\nestatistica_calculada = data %&gt;%\n  filter(str_detect(profissao, \"Médico\")) %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# gerando distribuição nula\ndistr_nula = data %&gt;%\n  filter(str_detect(profissao, \"Médico\")) %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 100, type = \"permute\") %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# calculando intervalo de confiança\npercentile_ci = get_ci(distr_nula)\n\n# visualizando distribuição nula e estatística do teste\ndistr_nula %&gt;%\n  visualize(method = \"both\") +\n  shade_p_value(estatistica_calculada, direction = \"greater\") +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n# calculando p-valor\ndistr_nula %&gt;%\n  get_p_value(obs_stat = estatistica_calculada, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nEm seguida, vamos calcular o tamanho do efeito.\n\n # calculando a diferença padronizada\n cohens_d(\n   data[data$sexo == \"homem\" &\n     str_detect(data$profissao, \"Médico\"), ]$rem_media,\n   data[data$sexo == \"mulher\" &\n     str_detect(data$profissao, \"Médico\"), ]$rem_media\n )\n\nCohen's d |       95% CI\n------------------------\n0.12      | [0.06, 0.19]\n\n- Estimated using pooled SD.\n\n # interpretando a estatística d\n interpret_cohens_d(0.12, \"gignac2016\")\n\n[1] \"very small\"\n(Rules: gignac2016)\n\n\nCom uma estatística d de 0.12, temos um efeito muito pequeno na escala de Gignac & Szodorai (2016). Isso significa que 54% das médicas da cidade de Vitória teriam remuneração abaixo do que o médico médio do município, enqaunto 46% receberiam mais que o médico médio."
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#disparidade-salarial-em-cargos-executivos",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#disparidade-salarial-em-cargos-executivos",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "3.2 DISPARIDADE SALARIAL EM CARGOS EXECUTIVOS",
    "text": "3.2 DISPARIDADE SALARIAL EM CARGOS EXECUTIVOS\nRepetindo o mesmo procedimento para as pessoas em cargos executivos, vemos que a diferença também é significativa.\n\n# calculando estatística t\nestatistica_calculada = data %&gt;%\n  filter(str_detect(profissao, \"Dirigentes|Diretor\")) %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# gerando distribuição nula\ndistr_nula &lt;- data %&gt;%\n  filter(str_detect(profissao, \"Dirigentes|Diretor\")) %&gt;%\n  specify(rem_media ~ sexo) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 100, type = \"permute\") %&gt;%\n  calculate(stat = \"t\", order = c(\"homem\", \"mulher\"))\n\n# calculando intervalo de confiança\npercentile_ci &lt;- get_ci(distr_nula)\n\n# visualizando distribuição nula e estatística do teste\ndistr_nula %&gt;%\n  visualize(method = \"both\") +\n  shade_p_value(estatistica_calculada, direction = \"greater\") +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\nE calculando o tamanho do efeito:\n\n # calculando a diferença padronizada\n cohens_d(\n   data[data$sexo == \"homem\" &\n     str_detect(data$profissao, \"Dirigentes|Diretor\"), ]$rem_media,\n   data[data$sexo == \"mulher\" &\n     str_detect(data$profissao, \"Dirigentes|Diretor\"), ]$rem_media\n )\n\nCohen's d |       95% CI\n------------------------\n0.47      | [0.30, 0.63]\n\n- Estimated using pooled SD.\n\n # interpretando a estatística d\n interpret_cohens_d(0.47, \"gignac2016\")\n\n[1] \"moderate\"\n(Rules: gignac2016)\n\n\nCom uma estatística d de 0.47, temos um efeito moderado na escala de Gignac & Szodorai (2016). Isso significa dizer que 69% das mulheres em posição de dirigentes e diretoras receberiam menos do que o homem médio na mesma posição."
  },
  {
    "objectID": "posts/202011_desigualdade-de-renda-entre-generos/index.html#footnotes",
    "href": "posts/202011_desigualdade-de-renda-entre-generos/index.html#footnotes",
    "title": "Effect Size e a desigualdade de renda entre gêneros em Vitória",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nEscolhi 2017 porque os dados mais atuais estão agrupados com Minas Gerais e Rio de Janeiro, ficando muito pesado para uma análise casual.↩︎\nCompreehensive R Archive Network.↩︎\nHá vários estudos que tratam disso, vide este exemplo.↩︎\nA tabela de interpretação de effect sizes pode ser consultada neste artigo.↩︎"
  },
  {
    "objectID": "posts/202009_hello-world/index.html",
    "href": "posts/202009_hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Depois de uma ou outra postagem no LinkedIn e Kaggle, já era hora de eu montar meu próprio blog para ciência de dados, não é mesmo?\nFirst things first, que nome é esse? Datamares & Dreamscapes faz referência ao título de uma coletânea de contos do Stephen King Nightmares & Dreamscapes (Pesadelos e Paisagens Noturnas, no Brasil) e faz bastante jus à minha relação com esse mundo. Eu smepre quero aprender mais, ir mais longe, conquistar novos horiontes, mas o aprendizado envolve certos sacrifícios e noites perdidas ou mal dormidas. Entretanto, quando você consegue aquele resultado esperado é como se um portal se abrisse para um novo mundo.\nEntão, sobre o que pretendo falar por aqui? Principalmente acerca dos projetos em R nos quais estiver trabalhando, machine learning, estatística clássica, forecasting, frameworks… e um pouco de matemática, economia e ensino. Basicamente ideias, insights e trabalhos que devo consultar vez ou outra.\nE se alguma dessas coisas te ajudar de alguma forma, eu vou ficar bastante feliz :p"
  },
  {
    "objectID": "posts/202208_template/index.html",
    "href": "posts/202208_template/index.html",
    "title": "Template para dissertações em Quarto",
    "section": "",
    "text": "Aproveitando que começo a escrever minha dissertação este semestre, porque não matar dois coelhos com uma cajadada só (Luisa Mell me perdoa) e conhecer esse tal de Quarto que está na boca do povo? Pois bem, aqui está um template para monografia/dissertação/tese em Quarto1.\nPara facilitar (ou não, o futuro dirá) a vida do povo, parametrizei os elementos pré-textuais — capa, folha de rosto, folha de aprovação, resumo, abstract etc. São esses aqui:\nPara usar, primeiramente os pré-requisitos:\nAgora você pode forkar o projeto e cair dentro! Basta atualizar os parâmetros em config/preamble.tex e escrever seu trabalho em dissertacao.qmd:"
  },
  {
    "objectID": "posts/202208_template/index.html#footnotes",
    "href": "posts/202208_template/index.html#footnotes",
    "title": "Template para dissertações em Quarto",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nEsse template é uma adaptação do modelo padrão da UEL.↩︎\nPara usuários de R, o mais fácil é via tinytex::install_tinytex().↩︎"
  },
  {
    "objectID": "posts/202106_python-from-r-i-package-importing/index.html",
    "href": "posts/202106_python-from-r-i-package-importing/index.html",
    "title": "Python a partir do R I: importando pacotes (e por que aprender novas linguagens é difícil)",
    "section": "",
    "text": "Resumo\n\n\n\nAo aprender uma nova linguagem de programação, simplesmente buscar código equivalente para as práticas que você já tem pode ser enganoso. Aqui vemos que um equivalente à chamada library() do R é, na verdade, considerada uma má prática em Python e, se você fizer isso em uma entrevista de emprego, não espere ser chamado de volta."
  },
  {
    "objectID": "posts/202106_python-from-r-i-package-importing/index.html#experiência-em-r",
    "href": "posts/202106_python-from-r-i-package-importing/index.html#experiência-em-r",
    "title": "Python a partir do R I: importando pacotes (e por que aprender novas linguagens é difícil)",
    "section": "2.1 Experiência Em R",
    "text": "2.1 Experiência Em R\nNo R, todo pacote instalado nas árvores de bibliotecas é listado sempre que um terminal é aberto. Esses pacotes listados estão disponíveis para os usuários o tempo todo e podem ser chamados explicitamente. Por exemplo:\n\n2.1.1 Caso 1: Chamada Explícita\n\n# procurar por medidas de machine learning que contenham \"AUC\" no pacote {mlr3}\nmlr3::mlr_measures$keys(\"auc\")\n\n[1] \"classif.auc\"       \"classif.mauc_au1p\" \"classif.mauc_au1u\"\n[4] \"classif.mauc_aunp\" \"classif.mauc_aunu\" \"classif.mauc_mu\"  \n[7] \"classif.prauc\"    \n\n\nMas esse modo de chamar funções geralmente só é usado se aquele pacote não será necessário com frequência. Caso contrário, é cultural para usuários de R carregar e anexar todo o namespace do pacote ao search path1 com uma chamada library().\n\n\n2.1.2 Caso 2: Anexando\n\n# cansado: chamada explícita do {ggplot2}\nt1 = mtcars |&gt;\n  dplyr::mutate(hp_by_cyl = hp / cyl)\n\n# animado: anexando o namespace do {ggplot2}\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nt2 = mtcars |&gt;\n  mutate(hp_by_cyl = hp / cyl)\n\n# são equivalentes?\nall.equal(t1, t2)\n\n[1] TRUE\n\n\nO problema aparece quando há conflitos de namespace. Você notou o aviso sobre objetos sendo mascarados de {stats} e {base}? Normalmente, os usuários simplesmente ignoram avisos de inicialização 😨 e isso pode eventualmente levar a resultados inconsistentes ou erros difíceis de depurar.\nIsso pode ser evitado anexando apenas as funções específicas que você realmente vai usar:\n\n\n2.1.3 Caso 3: Anexando Funções Específicas\n\n# desanexando dplyr\ndetach(\"package:dplyr\")\n\n# anexando apenas mutate():\nlibrary(dplyr, include.only = \"mutate\")\n\nE nenhum aviso de conflito será disparado. Infelizmente, não ouço muito falar do argumento include.only na comunidade R 🤷‍♂. Pelo contrário, meta pacotes como o {tidyverse}, que carregam e anexam MUITAS coisas ao namespace — muitas vezes desnecessárias para o que você vai fazer —, são bastante comuns."
  },
  {
    "objectID": "posts/202106_python-from-r-i-package-importing/index.html#experiência-em-python",
    "href": "posts/202106_python-from-r-i-package-importing/index.html#experiência-em-python",
    "title": "Python a partir do R I: importando pacotes (e por que aprender novas linguagens é difícil)",
    "section": "2.2 Experiência Em Python",
    "text": "2.2 Experiência Em Python\nTodos os 3 casos citados antes são possíveis em Python, mas os padrões da comunidade são bem diferentes. Especialmente em relação à consciência do que está carregado no namespace — ou symbol table, como é chamado em Python2.\nPrimeiro, pacotes instalados não ficam imediatamente disponíveis. Então, se eu tentar, por exemplo, listar funções/métodos/atributos do {pandas}, resultará em erro:\n\n# inspecionando módulos em {pandas}\nimport pandas\ndir(pandas)\n\n['ArrowDtype', 'BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt8Dtype', '__all__', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_built_with_meson', '_config', '_is_numpy_dev', '_libs', '_pandas_datetime_CAPI', '_pandas_parser_CAPI', '_testing', '_typing', '_version_meson', 'annotations', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'from_dummies', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n\n\nVocê pode checar a symbol table com o seguinte comando.\n\n# o que está anexado à symbol table?\nprint(*globals(), sep = \"\\n\")\n\n__name__\n__doc__\n__package__\n__loader__\n__spec__\n__annotations__\n__builtins__\nr\npandas\n\n\nDependendo do sistema/ferramentas que você está usando, o interpretador Python vai carregar alguns módulos ou não. Se você iniciar um REPL — um terminal interativo Python —, nenhum módulo será carregado. Se iniciar um Jupyter notebook, alguns módulos necessários para ele rodar serão carregados. Neste caso, como estou rodando Python a partir do R via {reticulate}, alguns módulos foram carregados:\n\nsys: para acesso a variáveis e funções usadas pelo interpretador\nos: para rotinas do sistema operacional NT ou Posix\n\nEntão, se quero trabalhar com {pandas}, preciso anexá-lo à symbol table com um equivalente ao library() do R. E assim como sua função prima, o import do Python também tem diferentes formas.\nPrimeiro, import pandas torna o pacote disponível para chamadas explícitas.\n\n# importar pandas\nimport pandas\n\n# o que está anexado à symbol table?\nprint(*globals(), sep = \"\\n\")\n\n__name__\n__doc__\n__package__\n__loader__\n__spec__\n__annotations__\n__builtins__\nr\npandas\n\n\nNote que apenas {pandas} está anexado à symbol table, não suas funções/métodos/atributos. Portanto, essa instrução não é equivalente ao library(). Para criar um dataframe simples com {pandas}:\n\n2.2.1 Caso 1: Chamada Explícita\n\n# isso resultará em NameError: name 'DataFrame' is not defined\nDataFrame(\n  {\n    \"capital\": [\"Vitoria\", \"São Paulo\", \"Rio de Janeiro\"],\n    \"state\": [\"Espírito Santo\", \"São Paulo\", \"Rio de Janeiro\"]\n  }\n)\n\nNameError: name 'DataFrame' is not defined\n\n# isso funciona\npandas.DataFrame(\n  {\n    \"capital\": [\"Vitoria\", \"São Paulo\", \"Rio de Janeiro\"],\n    \"state\": [\"Espírito Santo\", \"São Paulo\", \"Rio de Janeiro\"]\n  }\n)\n\n          capital           state\n0         Vitoria  Espírito Santo\n1       São Paulo       São Paulo\n2  Rio de Janeiro  Rio de Janeiro\n\n\nSe quisermos replicar o comportamento do library() (ou seja, carregar e anexar todas as funções/métodos/atributos do {pandas} à symbol table), então:\n\n\n2.2.2 Caso 2: Anexando\n\n# importando todo o {pandas} para a symbol table\nfrom pandas import *\n\n# symbol table atualizada\nprint(*globals(), sep = \"\\n\")\n\n__name__\n__doc__\n__package__\n__loader__\n__spec__\n__annotations__\n__builtins__\nr\npandas\nArrowDtype\nBooleanDtype\nCategorical\nCategoricalDtype\nCategoricalIndex\nDataFrame\nDateOffset\nDatetimeIndex\nDatetimeTZDtype\nExcelFile\nExcelWriter\nFlags\nFloat32Dtype\nFloat64Dtype\nGrouper\nHDFStore\nIndex\nIndexSlice\nInt16Dtype\nInt32Dtype\nInt64Dtype\nInt8Dtype\nInterval\nIntervalDtype\nIntervalIndex\nMultiIndex\nNA\nNaT\nNamedAgg\nPeriod\nPeriodDtype\nPeriodIndex\nRangeIndex\nSeries\nSparseDtype\nStringDtype\nTimedelta\nTimedeltaIndex\nTimestamp\nUInt16Dtype\nUInt32Dtype\nUInt64Dtype\nUInt8Dtype\napi\narray\narrays\nbdate_range\nconcat\ncrosstab\ncut\ndate_range\ndescribe_option\nerrors\neval\nfactorize\nget_dummies\nfrom_dummies\nget_option\ninfer_freq\ninterval_range\nio\nisna\nisnull\njson_normalize\nlreshape\nmelt\nmerge\nmerge_asof\nmerge_ordered\nnotna\nnotnull\noffsets\noption_context\noptions\nperiod_range\npivot\npivot_table\nplotting\nqcut\nread_clipboard\nread_csv\nread_excel\nread_feather\nread_fwf\nread_gbq\nread_hdf\nread_html\nread_json\nread_orc\nread_parquet\nread_pickle\nread_sas\nread_spss\nread_sql\nread_sql_query\nread_sql_table\nread_stata\nread_table\nread_xml\nreset_option\nset_eng_float_format\nset_option\nshow_versions\ntest\ntesting\ntimedelta_range\nto_datetime\nto_numeric\nto_pickle\nto_timedelta\ntseries\nunique\nvalue_counts\nwide_to_long\n\n# agora isso funciona\nDataFrame(\n  {\n    \"capital\": [\"Vitoria\", \"São Paulo\", \"Rio de Janeiro\"],\n    \"state\": [\"Espírito Santo\", \"São Paulo\", \"Rio de Janeiro\"]\n  }\n)\n\n          capital           state\n0         Vitoria  Espírito Santo\n1       São Paulo       São Paulo\n2  Rio de Janeiro  Rio de Janeiro\n\n\nMas você não verá nenhum usuário experiente de Python fazendo isso, pois eles se preocupam em carregar muitos nomes na symbol table e os possíveis conflitos que isso pode causar. Uma abordagem aceitável seria anexar apenas alguns nomes frequentes, como em:\n\n\n2.2.3 Caso 3: Anexando Funções Específicas\n\n# desanexando {pandas}\nfor name in vars(pandas):\n    if not name.startswith('_'):\n        del globals()[name]\n\nKeyError: 'annotations'\n\n# anexando apenas DataFrame()\nfrom pandas import DataFrame\n\n# symbol table atualizada\nprint(*globals(), sep = \"\\n\")\n\n__name__\n__doc__\n__package__\n__loader__\n__spec__\n__annotations__\n__builtins__\nr\npandas\nArrowDtype\nBooleanDtype\nCategorical\nCategoricalDtype\nCategoricalIndex\nDataFrame\nDateOffset\nDatetimeIndex\nDatetimeTZDtype\nExcelFile\nExcelWriter\nFlags\nFloat32Dtype\nFloat64Dtype\nGrouper\nHDFStore\nIndex\nIndexSlice\nInt16Dtype\nInt32Dtype\nInt64Dtype\nInt8Dtype\nInterval\nIntervalDtype\nIntervalIndex\nMultiIndex\nNA\nNaT\nNamedAgg\nPeriod\nPeriodDtype\nPeriodIndex\nRangeIndex\nSeries\nSparseDtype\nStringDtype\nTimedelta\nTimedeltaIndex\nTimestamp\nUInt16Dtype\nUInt32Dtype\nUInt64Dtype\nUInt8Dtype\napi\narray\narrays\nbdate_range\nconcat\ncrosstab\ncut\ndate_range\ndescribe_option\nerrors\neval\nfactorize\nget_dummies\nfrom_dummies\nget_option\ninfer_freq\ninterval_range\nio\nisna\nisnull\njson_normalize\nlreshape\nmelt\nmerge\nmerge_asof\nmerge_ordered\nnotna\nnotnull\noffsets\noption_context\noptions\nperiod_range\npivot\npivot_table\nplotting\nqcut\nread_clipboard\nread_csv\nread_excel\nread_feather\nread_fwf\nread_gbq\nread_hdf\nread_html\nread_json\nread_orc\nread_parquet\nread_pickle\nread_sas\nread_spss\nread_sql\nread_sql_query\nread_sql_table\nread_stata\nread_table\nread_xml\nreset_option\nset_eng_float_format\nset_option\nshow_versions\ntest\ntesting\ntimedelta_range\nto_datetime\nto_numeric\nto_pickle\nto_timedelta\ntseries\nunique\nvalue_counts\nwide_to_long\nname\n\n\nSegundo o The Hitchhiker’s Guide to Python [@pythonguide], caso 2 é o pior cenário possível e geralmente é considerado má prática, pois “torna o código mais difícil de ler e as dependências menos compartimentalizadas”. Essa afirmação é endossada pela documentação oficial do Python [@pythontutorial]:\n\nEmbora certos módulos sejam projetados para exportar apenas nomes que seguem certos padrões quando você usa import *, ainda é considerado má prática em código de produção.\n\nNa opinião dos autores do guia, caso 3 seria uma opção melhor porque destaca nomes específicos3, enquanto caso 1 seria a melhor prática, pois “Ser capaz de dizer imediatamente de onde uma classe ou função vem melhora muito a legibilidade e compreensão do código, exceto nos projetos mais simples de arquivo único.”"
  },
  {
    "objectID": "posts/202106_python-from-r-i-package-importing/index.html#footnotes",
    "href": "posts/202106_python-from-r-i-package-importing/index.html#footnotes",
    "title": "Python a partir do R I: importando pacotes (e por que aprender novas linguagens é difícil)",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUma lista ordenada onde o R procura por uma função. Pode ser acessada com search().↩︎\nAcho? Não sei, ainda estou aprendendo lol 😂↩︎\nPython Foundation diz “Não há nada de errado em usar from package import specific_submodule! Na verdade, essa é a notação recomendada, a menos que o módulo importador precise usar submódulos com o mesmo nome de pacotes diferentes.”↩︎"
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html",
    "href": "posts/202011_comparando-variancias/index.html",
    "title": "Comparando variâncias: o teste F",
    "section": "",
    "text": "Antes de eu entrar no assunto machine learning, porque é um buraco sem fundo devo me demorar quando entrar, quero cobrir um pouco mais do básico em inferência.\nNesse post eu disse que para realizar o teste t em duas amostras independentes deveríamos saber antes se as variâncias dessas amostras são iguais ou diferentes. Vamos ver como atestar isso agora. A base utilizada será a german credit data.\n# importando dados\n# obs.: o -1 é para remover a primeira coluna, que é apenas o índice\ndata = readr::read_csv(\"german_credit_data.csv\")[-1]\n\nNew names:\nRows: 1000 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(5): Sex, Housing, Saving accounts, Checking account, Purpose dbl (5): ...1,\nAge, Job, Credit amount, Duration\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# visualizando\nprint(data)\n\n# A tibble: 1,000 × 9\n     Age Sex      Job Housing `Saving accounts` `Checking account`\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;             \n 1    67 male       2 own     &lt;NA&gt;              little            \n 2    22 female     2 own     little            moderate          \n 3    49 male       1 own     little            &lt;NA&gt;              \n 4    45 male       2 free    little            little            \n 5    53 male       2 free    little            little            \n 6    35 male       1 free    &lt;NA&gt;              &lt;NA&gt;              \n 7    53 male       2 own     quite rich        &lt;NA&gt;              \n 8    35 male       3 rent    little            moderate          \n 9    61 male       1 own     rich              &lt;NA&gt;              \n10    28 male       3 own     little            moderate          \n# ℹ 990 more rows\n# ℹ 3 more variables: `Credit amount` &lt;dbl&gt;, Duration &lt;dbl&gt;, Purpose &lt;chr&gt;\nDa mesma forma que o teste t, podemos testar se a medida de uma amostra é significativamente diferente de um valor escolhido ou podemos testar em relação à outra amostra — se maior, menor ou diferente. Para este exercício, vamos testar se a variância da variável Credit amount (limite de crédito) é o mesmo para homens e mulheres que vivem de aluguel. Primeiro, vamos calcular os desvios-padrão populacionais:\n# obtendo amostras\nhomens = data[data$Sex == \"male\" & data$Housing == \"rent\",]$`Credit amount`\nmulheres = data[data$Sex == \"female\" & data$Housing == \"rent\",]$`Credit amount`\n\n# calculando desvio padrão\nsd(homens)\n\n[1] 2846.647\n\nsd(mulheres)\n\n[1] 2235.225\nVerificamos que o limite de crédito dos homens tem um desvio-padrão de DM$ 2.846, enquanto o das mulheres é de DM$ 22351, o que quer dizer que o limite de crédito dos homens varia mais em torno da média do que das mulheres. O que gostaríamos de saber agora é se essa diferença é significativamente diferente. Vamos ao teste!"
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#verificando-a-hipótese-de-normalidade",
    "href": "posts/202011_comparando-variancias/index.html#verificando-a-hipótese-de-normalidade",
    "title": "Comparando variâncias: o teste F",
    "section": "1.1 VERIFICANDO A HIPÓTESE DE NORMALIDADE",
    "text": "1.1 VERIFICANDO A HIPÓTESE DE NORMALIDADE\nPrimeiramente, vamos plotar as densidades para verificar se sua distribuição é plausível com a hipótese de normalidade:\n\n# dados de densidade\nd1 = density(homens)\nd2 = density(mulheres)\n\n# separando o grid em 2 colunas\npar(mfrow = c(1,2))\n\n# visualização\nplot(d1,\n  main = \"Density Plot: homens\")\npolygon(d1, col = \"lightblue\")\n\nplot(d2,\n  main = \"Density Plot: mulheres\")\npolygon(d2, col = \"salmon\")\n\n\n\n\n\n\n\n\nCom esse formato, a normalidade é bastante implausível e não há necessidade de realizar quaisquer testes. Para contornar esse problema, podemos tentar realizar uma transformação logarítmica:\n\n# transformação logarítimica\nlog_homens = log(homens)\nlog_mulheres = log(mulheres)\n\n# calculando a variância após transformação\nvar(log_homens)\n\n[1] 0.5614271\n\nvar(log_mulheres)\n\n[1] 0.5229548\n\n# dados de densidade\nd3 = density(log_homens)\nd4 = density(log_mulheres)\n\n# separando o grid em 2 colunas\npar(mfrow = c(1,2))\n\n# visualização\nplot(d3,\n  main = \"Density Plot: log(homens)\")\npolygon(d3, col = \"lightblue\")\n\nplot(d4,\n  main = \"Density Plot: log(mulheres)\")\npolygon(d4, col = \"salmon\")\n\n\n\n\n\n\n\n\nOs dados agora parecem seguir uma distribuição próxima da normal. Para verificar, pode-se realizar um teste de normalidade mas, como não é esse o tema, exploraremos o assunto em outra postagem. Por hora, vamos apenas registrar que a transformação foi exitosa e os dados agora apresentam uma distribuição próxima da normal.\n\n# teste de normalidade\nshapiro.test(log_homens)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_homens\nW = 0.98624, p-value = 0.5147\n\nshapiro.test(log_mulheres)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_mulheres\nW = 0.98171, p-value = 0.2071"
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#as-hipóteses",
    "href": "posts/202011_comparando-variancias/index.html#as-hipóteses",
    "title": "Comparando variâncias: o teste F",
    "section": "1.2 AS HIPÓTESES",
    "text": "1.2 AS HIPÓTESES\n\\[\n\\begin{cases}\nH_0: \\sigma_1 = \\sigma_2 \\\\\nH_1: \\sigma_1 \\neq \\sigma_2\n\\end{cases}\n\\]\nA hipótese nula é de que não se pode inferir, com certo nível de significância, que as variâncias são diferentes. E a hipótese alternativa é de que elas são significamente distintas."
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#nível-de-significância",
    "href": "posts/202011_comparando-variancias/index.html#nível-de-significância",
    "title": "Comparando variâncias: o teste F",
    "section": "1.3 NÍVEL DE SIGNIFICÂNCIA",
    "text": "1.3 NÍVEL DE SIGNIFICÂNCIA\n\\[ \\alpha = 0.05 \\]\nVamos utilizar um nível de significância padrão de 5%, o que quer dizer que a probabilidade de rejeitarmos a hipótese nula quando ela não deve ser rejeitada é de apenas 5%. Quanto menor for essa probabilidade, maior deve ser a diferença entre as variâncias para que possamos atestar a diferença significativa entre elas."
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#estatística-do-teste",
    "href": "posts/202011_comparando-variancias/index.html#estatística-do-teste",
    "title": "Comparando variâncias: o teste F",
    "section": "1.4 ESTATÍSTICA DO TESTE",
    "text": "1.4 ESTATÍSTICA DO TESTE\n\\[ F = \\frac{s^2_1}{s^2_2} \\]\nComo a estatística do teste é a razão entre as variâncias amostrais, o teste é para verificar se essa razão é diferente da unidade. Para verificarmos a estatística tabelada, precisamos de saber quantos graus de liberdade temos nas amostras:\n\n# graus de liberdade (n-1)\ntable(data[data$Housing == \"rent\",]$Sex)\n\n\nfemale   male \n    95     84 \n\n\nE então a estatística tabelada será:\n\n# F-statistic para 95º percentil\nqf(.95, 83, 94)\n\n[1] 1.419123"
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#valor-crítico",
    "href": "posts/202011_comparando-variancias/index.html#valor-crítico",
    "title": "Comparando variâncias: o teste F",
    "section": "1.5 VALOR CRÍTICO",
    "text": "1.5 VALOR CRÍTICO\n\n# calculando valor crítico\nvar(log_homens) / var(log_mulheres)\n\n[1] 1.073567\n\n\n\\[ F = \\frac{s^2_1}{s^2_2} = 1.07 \\]"
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#decisão",
    "href": "posts/202011_comparando-variancias/index.html#decisão",
    "title": "Comparando variâncias: o teste F",
    "section": "1.6 DECISÃO",
    "text": "1.6 DECISÃO\nComo o valor de 1.07 não excede 1.42, não podemos rejeitar a hipótese nula ao nível de 5% de significância. As variâncias não são significativamente distintas."
  },
  {
    "objectID": "posts/202011_comparando-variancias/index.html#footnotes",
    "href": "posts/202011_comparando-variancias/index.html#footnotes",
    "title": "Comparando variâncias: o teste F",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nMarcos alemães.↩︎"
  },
  {
    "objectID": "posts/202011_t-test/index.html",
    "href": "posts/202011_t-test/index.html",
    "title": "Atestando diferenças em médias: o teste t para amostras independentes",
    "section": "",
    "text": "Suponha que você tenha duas amostras (i.e. as rendas da população negra e branca de sua cidade) e você queira comprovar que suas médias sejam significantemente diferentes, ou seja, que sejam diferentes mesmo considerando a variância e o tamanho das amostras. Isso é possível com um teste t de Student, um dos mais populares testes na estatística.\nVamos utilizar um dos datasets nativos do R para aplicar esse conceito, o mtcars. Primeiramente, vamos dar uma olhada em nossos dados.\n\ndata = mtcars\nknitr::kable(head(data), booktabs = TRUE, digits = 2) |&gt;\n  kableExtra::kable_styling(latex_options = c(\"striped\"))\n\n\n\nTabela 1: Base de dados.\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\nUma boa forma de ilustrar o teste é verificar se as médias de consumo dos carros (mpg, miles per gallon) com 4, 6 e 8 cilindros (cyl) diferem significantemente entre si.\n\n# Médias amostrais\naggregate(mpg ~ cyl, data = data, FUN = mean)\n\n  cyl      mpg\n1   4 26.66364\n2   6 19.74286\n3   8 15.10000\n\n\nVerificamos que as médias amostrais são diferentes. Resta saber se são significantemente diferentes. Plotar um boxplot pode nos ajudar a ter uma intuição. Podemos ver que, exceto pelo grupo de 4 cilindros que possui uma variância maior, os grupos são bem concentrados, de forma que podemos suspeitar que as diferenças sejam significantes.\n\n# Boxplot\nboxplot(mpg ~ cyl, data = data)\n\n\n\n\n\n\n\nFigura 1: Boxplot de consumo por quantidade de cilindros.\n\n\n\n\n\nO teste t possui diversas variações — uma amostra, duas amostras pareadas, duas amostras independentes —, e correções para tratar diferenças na variância. Para este caso, temos três amostras independentes e, por hora, vamos supor que as variâncias do grupo 4 difere das demais e que a dos grupos 6 e 8 são iguais — deixaremos a análise de variâncias para outra postagem. Isso nos deixa com o teste t para duas amostras independentes.\nA hipótese nula do teste é que as médias são significativamente iguais. Já a hipótese alternativa pode ser formulada como a não nulidade da diferença entre as médias ou \\(\\bar{X_1}\\) maior ou menor que \\(\\bar{X_2}\\). Aqui vamos usar a primeira opção:\n\\[\nh_0: \\bar{X_1} - \\bar{X_2} = 0 \\\\\nh_1: \\bar{X_1} - \\bar{X_2} \\neq 0\n\\] A estatística t para esse teste é calculada da maneira abaixo. Note que se tomarmos o limite de \\(t(n)\\), com \\(n \\rightarrow \\infty\\), \\(t\\rightarrow \\infty\\), causando a rejeição de \\(h_0\\). Dessa forma, em último caso, o teste t é um teste de tamanho de amostra, ou seja, se sua amostra for suficientemente grande e as médias divergirem, elas tenderão a ser também significantemente diferentes.\n\\[ t = \\frac{\\bar{X_1} - \\bar{X_2}}{s_p . \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}} \\]\nComo primeiro caso, vamos comparar as médias de consumo dos veículos com 6 e 8 cilindros. Como estamos considerando que suas variâncias são iguais, temos de usar o argumento var.equal = TRUE:\n\n# teste t para 6 e 8 cilindros\nt.test(mpg ~ cyl, data = data[which(data$cyl != 4),], var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  mpg by cyl\nt = 4.419, df = 19, p-value = 0.0002947\nalternative hypothesis: true difference in means between group 6 and group 8 is not equal to 0\n95 percent confidence interval:\n 2.443809 6.841905\nsample estimates:\nmean in group 6 mean in group 8 \n       19.74286        15.10000 \n\n\nCom p-valor de zero, podemos rejeitar a hipótese nula e considerar que as médias de consumo entre os veículos de 6 e 8 cilindros diferem.\nPara as demais comparações, vamos usar o default para var.equal que é FALSE. Isso significa aplicar a correção de Welch para amostras independentes e de variância diferentes. Como esperado, também podemos rejeitar a hipótese nula e confirmar a diferença nas médias de consumo entre os veículos de 4 e 6 cilindros e 4 e 8 cilindros.\n\n# teste t para 4 e 8 cilindros\nt.test(mpg ~ cyl, data = data[which(data$cyl != 6),])\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by cyl\nt = 7.5967, df = 14.967, p-value = 1.641e-06\nalternative hypothesis: true difference in means between group 4 and group 8 is not equal to 0\n95 percent confidence interval:\n  8.318518 14.808755\nsample estimates:\nmean in group 4 mean in group 8 \n       26.66364        15.10000 \n\n# teste t para 4 e 6 cilindros\nt.test(mpg ~ cyl, data = data[which(data$cyl != 8),])\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by cyl\nt = 4.7191, df = 12.956, p-value = 0.0004048\nalternative hypothesis: true difference in means between group 4 and group 6 is not equal to 0\n95 percent confidence interval:\n  3.751376 10.090182\nsample estimates:\nmean in group 4 mean in group 6 \n       26.66364        19.74286 \n\n\nEasy peasy lemon squeezy, não é?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Atuo na modelagem de risco de crédito e ciências de dados em geral no Banco do Estado do Espírito Santo (@Banestes_SA), onde sou concursado desde 2008. Meu foco está em aumentar a qualidade da informação e reduzir a incerteza da tomada de decisão através de métodos estatísticos e experimentação. Sou economista (Universidade Federal do Espírito Santo - UFES, 2014) e matemático (Instituto Federal do Espírito Santo, 2024) de formação, e me tornei cientista de dados por afinidade.\nSou discente de doutorado no Programa de Pós-Graduação da Universidade Federal do Espírito Santo (PPGEco/UFES), onde também cursei meu mestrado. Meu campo de pesquisa é econometria de séries temporais, especificamente séries temporais hierárquicas e métodos de aprendizado de máquina para previsão de séries temporais hierárquicas.\nEm relação à tecnologias, eu sou um entusiasta de FOSS (Free and Open Source Software) e utilizo principalmente R, Rust e Python em meus projetos. Não uso um único OS e estou sempre variando entre macOS, Linux Ubuntu e Linux Fedora em casa, e Windows no trabalho. Sou autor do pacote fio, que é uma ferramenta para análise de matrizes insumo-produto – que combina perfeitamente meu background em economia, matemática e computação.\nEm meu tempo livre eu gosto de surfar, tocar guitarra e produzir músicas em meu home studio."
  }
]